{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0488181f-0792-4bd2-8a7d-15408d8d4804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (0.2.9)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from requests->kagglehub) (2024.12.14)\n",
      "Requirement already satisfied: contractions in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "Requirement already satisfied: num2words in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (0.5.14)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from num2words) (0.6.2)\n",
      "Requirement already satisfied: inflect in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (7.4.0)\n",
      "Requirement already satisfied: more-itertools>=8.5.0 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from inflect) (10.3.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from inflect) (4.4.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from inflect) (4.12.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from typeguard>=4.0.1->inflect) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from importlib-metadata>=3.6->typeguard>=4.0.1->inflect) (3.20.2)\n",
      "Requirement already satisfied: jieba in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (0.42.1)\n",
      "Requirement already satisfied: cn2an in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (0.5.23)\n",
      "Requirement already satisfied: proces>=0.1.7 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from cn2an) (0.1.7)\n",
      "Requirement already satisfied: sacrebleu in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (2.5.1)\n",
      "Requirement already satisfied: portalocker in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from sacrebleu) (3.0.0)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from sacrebleu) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from sacrebleu) (1.24.4)\n",
      "Requirement already satisfied: colorama in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from sacrebleu) (5.3.0)\n",
      "Requirement already satisfied: safetensors in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (0.4.5)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub\n",
    "!pip install contractions\n",
    "!pip install num2words\n",
    "!pip install inflect\n",
    "!pip install jieba\n",
    "!pip install cn2an\n",
    "!pip install sacrebleu\n",
    "!pip install safetensors\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d372e6f2-7e93-4b37-ba45-5559c9af8dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/telegram-grammar-bot/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "\n",
    "import num2words\n",
    "import cn2an\n",
    "import jieba\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from num2words import num2words\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82fec4ca-53a7-4a37-9653-2d46d227229a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869f7b71-011b-4fa1-a7c8-5794175eb627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'\n",
    "weights_path = '/content/drive/MyDrive/Colab Notebooks/model_weights/model.safetensors'\n",
    "state_dict = load_file(weights_path)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name,state_dict=state_dict)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, state_dict=state_dict)\n",
    "\n",
    "prefix = 'translate to zh: '\n",
    "src_text = prefix + \"1. Яблоко.\"\n",
    "\n",
    "# translate Russian to Chinese\n",
    "input_ids = tokenizer(src_text, return_tensors=\"pt\")\n",
    "\n",
    "generated_tokens = model.generate(**input_ids)\n",
    "\n",
    "result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "print(result)\n",
    "#开发的目的是为用户提供个人同步翻译。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e6f932-8e97-41d3-9624-689b4430ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "\n",
    "class CleanChineseText:\n",
    "    def __init__(self, zh_text):\n",
    "        self.zh_text = zh_text\n",
    "    def escape_en_char(self):\n",
    "        en_pattern = \"[a-zA-Z]+\"\n",
    "        self.zh_text = re.sub(en_pattern, ' ', self.zh_text)\n",
    "        return self\n",
    "    def remove_enter_and_symbols(self):\n",
    "        self.zh_text = re.sub(r'^\\d+\\.', '', self.zh_text).strip()\n",
    "        return self\n",
    "    def remove_gaps(self):\n",
    "        self.zh_text = self.zh_text.replace(' ', '')\n",
    "        return self\n",
    "    def remove_punct(self):\n",
    "        self.zh_text = self.zh_text.strip('，')\n",
    "        return self\n",
    "\n",
    "    def clean_all(self):\n",
    "        self.remove_punct()\n",
    "        self.remove_enter_and_symbols()\n",
    "        self.escape_en_char()\n",
    "        self.remove_gaps()\n",
    "        return self.zh_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefd16e1-e7fa-4821-b703-797c252b2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CleanRussianText:\n",
    "    def __init__(self, ru_text):\n",
    "        self.ru_text = ru_text\n",
    "    # def to_one_style(self):\n",
    "    #     # Lower case\n",
    "    #     self.ru_text = self.ru_text.lower()\n",
    "    #     return self\n",
    "    # def remove_numbers_enters_symbols(self):\n",
    "    #     # remove all symbols, numbers and /n in the end of the strings\n",
    "    #     self.ru_text = re.sub(r'[^\\w\\s]', '', self.ru_text).strip()\n",
    "    #     # self.ru_text = re.sub(r'^\\d+\\.', '', self.ru_text).strip()\n",
    "    #     # self.ru_text = re.sub('[^а-яА-Я0-9\\s]', ' ', self.ru_text)\n",
    "    #     return self\n",
    "\n",
    "    # def combine_numbers(self):\n",
    "    #     # combine numbers with a space, such as '40 000' to '40000'\n",
    "    #     self.ru_text = re.sub(r'(\\d+)\\s*(\\d+)', r'\\1\\2', self.ru_text)\n",
    "    #     return self\n",
    "    # def separate_numbers_and_words(self):\n",
    "    #     # separate words like '11летнему' to '11 летнему'\n",
    "    #     self.ru_text = re.sub(r'(\\d+)([а-яА-Я]+)', r'\\1 \\2', self.ru_text)\n",
    "    #     return self\n",
    "\n",
    "    # def numbers_to_words(self):\n",
    "    #     # convert numbers to words '40000' to 'сорок тысяч'\n",
    "    #     def convert_numbers(match):\n",
    "    #       return num2words(int(match.group()), lang='ru', to='ordinal')\n",
    "    #     self.ru_text = re.sub(r'\\d+', convert_numbers, self.ru_text)\n",
    "    #     return self\n",
    "\n",
    "    def clean_all(self):\n",
    "      self.ru_text = self.ru_text.lower().strip()\n",
    "      self.ru_text = re.sub(r'\\s+', ' ', self.ru_text)\n",
    "      self.ru_text = re.sub(r'[^\\w\\s]', '', self.ru_text)\n",
    "      self.ru_text = re.sub(r'(\\d+)\\s*(\\d+)', r'\\1\\2', self.ru_text)\n",
    "      # self.combine_numbers()\n",
    "      # self.separate_numbers_and_words()\n",
    "      # self.numbers_to_words()\n",
    "      return self.ru_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b43a9-ec51-4d48-8fd8-0b008a6681a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' ♫ как лёд сходит и 3 000 лет идет к 11летнему Тиму ♫'\n",
    "cleaner = CleanRussianText(text)\n",
    "result = cleaner.clean_all()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b981b-1ae6-49fe-9799-4fa2a7b3d0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '，我是1个anzh很好好的15岁的孩子，'\n",
    "cleaner = CleanChineseText()\n",
    "result = cleaner.clean_all()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69813ea6-a644-4c15-aa86-e517fc22a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(source_ru, target_zh, tokenizer=tokenizer):\n",
    "    prefix = 'translate to zh'\n",
    "    source_ru_texts = [f'{prefix}: {text}' for text in source_ru]\n",
    "    tokenized_ru_texts = [tokenizer(text, truncation=True, padding=True, return_tensors='pt') for text in source_ru_texts]\n",
    "\n",
    "    target_zh_texts = [tokenizer(zh_text, truncation=True, padding=True, return_tensors='pt') for zh_text in target_zh]\n",
    "    return tokenized_ru_texts, target_zh_texts\n",
    "\n",
    "class TokenizedT5CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_ru_texts_ids, target_zh_texts_ids):\n",
    "        self.source_ru = tokenized_ru_texts_ids\n",
    "        self.target_zh = target_zh_texts_ids\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.source_ru[idx]['input_ids'].squeeze(0),\n",
    "            'attention_mask': self.source_ru[idx]['attention_mask'].squeeze(0),\n",
    "            'labels': self.target_zh[idx]['input_ids'].squeeze(0)\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.source_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab91b420-091c-4b6f-a215-c12e7887984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle_dataset = load_dataset(\"open_subtitles\", lang1=\"ru\", lang2=\"zh_cn\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e2a1a-6ab2-4a89-ba56-1eec44df73d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(subtitle_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9650b3c1-3f91-487a-97c5-d3ed3a35c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_ru'] = df['translation'].apply(lambda x: CleanRussianText(x['ru']).clean_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5341ef91-a8af-45d0-b445-f83b82832b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_zh'] = df['translation'].apply(lambda x: CleanChineseText(x['zh_cn']).clean_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af950be9-177c-4101-a356-10eb6418bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['translation'][934]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d83eb0e-155f-45c6-ba7d-77fe7b53d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "source_ru_train_subt, source_ru_val_subt, target_zh_train_subt, target_zh_val_subt = train_test_split(\n",
    "    df['cleaned_ru'], df['cleaned_zh'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8cd15-c5e4-462a-93bb-b11bdb379fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ru_texts_train, target_zh_texts_train = tokenize_dataset(source_ru_train_subt, target_zh_train_subt)\n",
    "tokenized_ru_texts_val, target_zh_texts_val = tokenize_dataset(source_ru_val_subt, target_zh_val_subt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d90af-0844-4a07-bc6f-44b312479fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_ru_texts_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79036dc8-6523-4be8-b47f-da343bf077e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_zh_texts_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f159ad0-3b62-440f-8581-6934092d3188",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_ru_texts_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5a852-3a9c-43e3-b5fc-cf651f563981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_zh_texts_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc34ee13-294d-4915-94de-b3cfe8848884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "train_dataset = TokenizedTedCustomDataset(tokenized_ru_texts_ids=tokenized_ru_texts_train, target_zh_texts_ids=target_zh_texts_train)\n",
    "test_dataset = TokenizedTedCustomDataset(tokenized_ru_texts_ids=tokenized_ru_texts_val, target_zh_texts_ids=target_zh_texts_val)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=256, shuffle=False, collate_fn=data_collator)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=256, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acefa00c-5b6d-4f20-b05e-a650ecaff22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f3a3f2-874f-40d8-a5db-21d9e49ff46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def bleu_score(model, tokenizer, eval_pred_loader):\n",
    "  actual, predicted = [], []\n",
    "  model.to('cuda')\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "\n",
    "    for batch in eval_pred_loader:\n",
    "      input_ids = batch['input_ids'].to('cuda')\n",
    "      attention_mask = batch['attention_mask'].to('cuda')\n",
    "      labels = batch['labels'].to('cuda')\n",
    "\n",
    "      outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "      decoded_labels = tokenizer.batch_decode(\n",
    "          [[token if token != -100 else tokenizer.pad_token_id for token in label] for label in labels],\n",
    "          skip_special_tokens=True,\n",
    "      )\n",
    "\n",
    "      predictions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "      actual.append([decoded_labels[0]])\n",
    "      predicted.append(predictions[0])\n",
    "\n",
    "  bleu_dic = {}\n",
    "  bleu_dic['bleu'] = corpus_bleu(actual, predicted)\n",
    "  # bleu_dic['1-2-grams'] = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
    "  # bleu_dic['1-3-grams'] = corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0), smoothing_function=smoothing)\n",
    "  # bleu_dic['1-4-grams'] = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "\n",
    "  return bleu_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af37c1ec-4b43-4ac5-a98a-136f3aa2fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_test_subt = bleu_score(model, tokenizer, test_loader)\n",
    "print(bleu_test_subt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e3e88-6628-4139-bdbe-f9c7d98c3ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/result',\n",
    "    evaluation_strategy='steps',\n",
    "    per_device_train_batch_size=512,\n",
    "    per_device_eval_batch_size=512,\n",
    "    learning_rate=5e-05,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553f7f9-d9f5-45f9-836a-2927663a65d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e70b04-75aa-44fb-abc6-1ecd23034093",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_test_subt_after_test = bleu_score(model, tokenizer, test_loader)\n",
    "print(bleu_test_subt_after_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
